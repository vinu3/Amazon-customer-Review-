# -*- coding: utf-8 -*-
"""Amazon_Different_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gwa4mwQl8kVPIkw_PYPwFMwBScYwHnkB

# **Authors**

**Team 03**

**Team Members:**

**Different Models**


*   Radhika Malviya
*   Silky Shah
*   Vinay Vihari Lakamsani
*   Janani Vasudevan
*   Xiaoxu Xing

# **Amazon customer review **
"""

# basic data manipulation and import
import pandas as pd
#plotting the graph
import matplotlib.pyplot as plt
# Matrix multiplication
import matplotlib as mpl
# natural Language Processing utilities
import nltk.classify.util
# Splitting data for test and train sets
from sklearn.model_selection import train_test_split
# for making the confusion matrix
from sklearn.metrics import confusion_matrix
# for calcualting matrics
from sklearn import metrics
# For making ROC_Curve and AUC
from sklearn.metrics import roc_curve, auc
# Importing naiveBayesClassifier
from nltk.classify import NaiveBayesClassifier
# Importing numpy for mathematical function
import numpy as np
# finding the matching String with the \
import re
import string
import nltk
# %matplotlib inline

"""# **Importing the Data**

Imported data from google drive and made as dowloadable link
"""

# importing the URL
url = "https://drive.google.com/uc?export=download&id=1k8rHxOQ7StKMuVKadsEEqy-YkzH0eyA5"
df =  pd.read_csv(url)
df.head(1)

"""# **Data Pre-Processing**

## Dropping unwanted variables
"""

# useful variables which are going to be used in further process
permanent = df[['reviews.rating','reviews.text','reviews.title']]
permanent.head()

"""## **Filtering not null values**"""

# check is our y Variable
check = permanent[permanent["reviews.rating"].notnull()]
check.head()

"""## **Classifying text as postive and negative**

*   Making the reviews greater than 4 as positive and rest as negative Reviews
*   saving this to senti dataframe
"""

senti= permanent[permanent["reviews.rating"].notnull()]
# makes the Reviews > 4 as the TRUE and rest as the FALSE
senti["senti"] = senti["reviews.rating"]>=4
#senti["senti"] = senti.loc[,'reviews.rating' >=4]
# Makes the review from TRUE FaLSE to POS and NEG
senti["senti"] = senti["senti"].replace([True,False],["pos","neg"])
senti.head()

print(senti.head())

"""## **Count of reviews**"""

#counting each set of pos and neg : then plotting the graph
senti["senti"].value_counts().plot.bar()

"""As we can see data is unbalanced so this will create problem for model but, will take this data as it is and will predict our reviews.

## **Cleaning the Review text**

*  we will clean the reviews text and save it as the  summary clean text in new column which makes easy to model
*  cleaning has three steps 1. converting to string 2. to lower case 3. removing other than A to z character like symbols
"""

# saving below pattern match
cleanup_re =re.compile('[^a-z]+')

#cleaning each sentence of reviews
def cleanup(sentence) :
  sentence = str(sentence)
  sentence = sentence.lower()
  sentence = cleanup_re.sub(' ',sentence).strip()
  return sentence

senti["Summary_Clean"] = senti["reviews.text"].apply(cleanup)
check["Summary_Clean"] = check["reviews.text"].apply(cleanup)
senti.head()

"""# **Splitting Train and Test Data**

*  Considering only the cleaned text and respective sentiment
*  75% as the training and 25% as the test dataset
"""

split =senti[["Summary_Clean","senti"]]
#Making train and test sets
train = split.sample(frac=0.75,random_state=3)
# dropping trainsets
test=split.drop(train.index)
train.head()

"""# **NLTK Navie Baies Classifier**

## **Feature Extractor **

Converting the sentence into list of words
"""

#Stemming to each sentence
from nltk.stem import PorterStemmer
ps = PorterStemmer()

import nltk
nltk.download('wordnet')

# lemmatization to each dsentence
from nltk.stem.wordnet import WordNetLemmatizer
lmtzr = WordNetLemmatizer()

def word_feats(words):
  features = {}
  for word in words:
    #word=ps.stem(word)
    #print("after steeming:" + word)
    word= lmtzr.lemmatize(word)
    #print("after lemtixingn:" +word)
    features [word] = True
  return features

ef =str("i am the good nice large larger run running")
efa = ef.split()
word_feats(efa)

train["words"] = train["Summary_Clean"].str.lower().str.split()
test["words"] = test["Summary_Clean"].str.lower().str.split()
check["words"] = check["Summary_Clean"].str.lower().str.split()
#train.head()
check.head()

"""Indexing all the datasets"""

# setting the index to the all seets
train.index = range(train.shape[0])
test.index  = range(test.shape[0])
check.index = range(check.shape[0])

train.head()

train.shape[0]

"""Assining the Pos and Neg values form senti to each word on the review text

## **Text to Words**
"""

train["words"][0]

prediction = {}
train_naive = []
test_naive = []
check_naive = []

#splitig each sentence and adding with True to the string
for i in range(train.shape[0]):
    train_naive = train_naive +[[word_feats(train["words"][i]) , train["senti"][i]]]
for i in range(test.shape[0]):
    test_naive = test_naive +[[word_feats(test["words"][i]) , test["senti"][i]]]
for i in range(check.shape[0]):
    check_naive = check_naive +[word_feats(check["words"][i])]

train_naive[0]

"""## **Model Fitting**"""

# classifying based on the Naive bayes Classifier
classifier = NaiveBayesClassifier.train(train_naive)
acc =nltk.classify.util.accuracy(classifier , test_naive)
print("NLTK Naive bayes Accuracy : " +str(acc))
classifier.show_most_informative_features(5)

"""**Predicting result of nltk classifier**"""

#classifying for training set
y =[]
only_words= [test_naive[i][0] for i in range(test.shape[0])]
for i in range(test.shape[0]):
    y = y + [classifier.classify(only_words[i] )]
prediction["Naive"]= np.asarray(y)

prediction["Naive"]

#classifying for the test set
y1 = []
for i in range(check.shape[0]):
    y1 = y1 + [classifier.classify(check_naive[i] )]

check["Naive"] = y1
check["Naive"]

"""# **Bulding Countvector and TFIDF vector **"""

# import stopwords from word cloud
from wordcloud import STOPWORDS

# importing Term Frequency Extraction form sklearn
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer

# stopwords removing Not from it
stopwords = set(STOPWORDS)
stopwords.remove("not")

count_vect = CountVectorizer(min_df=2 ,stop_words=stopwords , ngram_range=(1,2))

tfidf_transformer = TfidfTransformer()

X_train_counts = count_vect.fit_transform(train["Summary_Clean"])        
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)


X_new_counts = count_vect.transform(test["Summary_Clean"])
X_test_tfidf = tfidf_transformer.transform(X_new_counts)

checkcounts = count_vect.transform(check["Summary_Clean"])
checktfidf = tfidf_transformer.transform(checkcounts)

"""# **Fitting Multinomial NB**"""

from sklearn.naive_bayes import MultinomialNB
model1 = MultinomialNB().fit(X_train_tfidf , train["senti"])
prediction['Multinomial'] = model1.predict_proba(X_test_tfidf)[:,1]
acc = model1.score(X_test_tfidf , test["senti"])
print("Multinomial Accuracy : "+str(acc))

check["multi"] = model1.predict(checktfidf)## Predicting Sentiment for Check which was Null values for rating

"""# **Fitting Bernouli NB**"""

from sklearn.naive_bayes import BernoulliNB
#fitting the bernouli Naive Bayes
model2 = BernoulliNB().fit(X_train_tfidf,train["senti"])
prediction['Bernoulli'] = model2.predict_proba(X_test_tfidf)[:,1]
acc= model2.score(X_test_tfidf , test["senti"])
print("Bernoulli Accuracy : "+str(acc))

check["Bill"] = model2.predict(checktfidf)## Predicting Sentiment for Check which was Null values for rating

"""# **Fitting Logistic Regression**"""

from sklearn import linear_model
logreg = linear_model.LogisticRegression(solver='lbfgs' , C=1000)
logistic = logreg.fit(X_train_tfidf, train["senti"])
prediction['LogisticRegression'] = logreg.predict_proba(X_test_tfidf)[:,1]
acc=logreg.score(X_test_tfidf , test["senti"])
print("Logistic Regression Accuracy : "+str(acc))

check["log"] = logreg.predict(checktfidf)## Predicting Sentiment for Check which was Null values for rating

"""# **Getting most occuring words in train set**"""

words = count_vect.get_feature_names()
feature_coefs = pd.DataFrame(
    data = list(zip(words, logistic.coef_[0])),
    columns = ['feature', 'coef'])
feature_coefs.sort_values(by="coef")

"""# **comparing all models**

## ROC curves
"""

def formatt(x):
    if x == 'neg':
        return 0
    if x == 0:
        return 0
    return 1
vfunc = np.vectorize(formatt)

cmp = 0
colors = ['b', 'g', 'y', 'm', 'k']
for model, predicted in prediction.items():
    if model not in 'Naive':
        false_positive_rate, true_positive_rate, thresholds = roc_curve(test["senti"].map(vfunc), predicted)
        roc_auc = auc(false_positive_rate, true_positive_rate)
        plt.plot(false_positive_rate, true_positive_rate, colors[cmp], label='%s: AUC %0.2f'% (model,roc_auc))
        cmp += 1

plt.title('Classifiers comparaison with ROC')
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([-0.1,1.2])
plt.ylim([-0.1,1.2])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""*  Logistic NB performs better than the remaining all models.
*  Bernouli and Multinomial NB has similar performance.

## Precision, Recall, F1-score and support
"""

test.senti = test.senti.replace(["pos" , "neg"] , [True , False] )
keys = prediction.keys()
for key in ['Multinomial', 'Bernoulli', 'LogisticRegression']:
    print(" {}:".format(key))
    print(metrics.classification_report(test["senti"], prediction.get(key)>.5, target_names = ["positive", "negative"]))
    print("\n")

"""# **Testing with hand written samples**"""

def test_sample(model, sample):
    sample_counts = count_vect.transform([sample])
    sample_tfidf = tfidf_transformer.transform(sample_counts)
    result = model.predict(sample_tfidf)[0]
    prob = model.predict_proba(sample_tfidf)[0]
    print("Sample estimated as %s: negative prob %f, positive prob %f" % (result.upper(), prob[0], prob[1]))

test_sample(logreg, "this phone has awesome performance but it dont not have bluetooth")
test_sample(logreg, "bose is the best product i have seen and used in my life")
test_sample(logreg, "I dont like this product looks like fake")

"""# **Exploring more on dataset**

# **Forming word Cloud**

### All words from reviews
"""

from wordcloud import WordCloud, STOPWORDS
stopwords = set(STOPWORDS)


mpl.rcParams['font.size']=12                #10 
mpl.rcParams['savefig.dpi']=100             #72 
mpl.rcParams['figure.subplot.bottom']=.1 

def show_wordcloud(data, title = None):
    wordcloud = WordCloud(
        background_color='Orange',
        stopwords=stopwords,
        max_words=300,
        max_font_size=50, 
        scale=3,
        random_state=2 # chosen at random by flipping a coin; it was heads
        
    ).generate(str(data))
    
    fig = plt.figure(1, figsize=(15, 15))
    plt.axis('off')
    if title: 
        fig.suptitle(title, fontsize=20)
        fig.subplots_adjust(top=2.3)

    plt.imshow(wordcloud)
    plt.show()
    
show_wordcloud(senti["Summary_Clean"])

"""### Positive words from reviews"""

show_wordcloud(senti["Summary_Clean"][senti.senti == "pos"] , title="Postive Words")

"""### Negative words from reviews"""

show_wordcloud(senti["Summary_Clean"][senti.senti == "neg"] , title="Negitive words")

"""# Exploring more on columns"""

df.head(1)

"""**Unique values**"""

df.nunique()

"""1.  Id is not used
2.  **Name is used **: 48 different product are available
3.  Asins: Not used
4.  Brand: Not used
5.  **Categories**: 41 different product categories are available
6.  Keys: Not useful (similar to categories)
7.  Manufacturer: Mostly Amazon so no Problem

**Reviews**
8.  date: used(convert to dateformat)
9.  **dateadded:** useful when to check when reviews are given(convert to dateformat)
10.  dateseen: Not useful
11.  didPurchase: Not useful
12.  doRecommend: Not useful
13.  Id: Not useful
14.  numHelpful: Not useful
15. ** rating: higly useful**
16.  sourceURLS: Not useful
17.  **Text: useful** (but preprocess for modelling)
18.  **Title: veryuseful **
19.  userCity: Not used as there are no values
20.  userProvince: Not used as there are no values
21.  username : Not used
"""